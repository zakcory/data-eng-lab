{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976e66de23bfce85",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f09c59a02a3b0d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2980afd53a868188",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def semi_optimized_exhaustive_search(\n",
    "        index_vectors: np.ndarray,\n",
    "        query_vectors: np.ndarray,\n",
    "        k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function performs an optimized exhaustive search.\n",
    "    Args:\n",
    "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
    "        query_vectors: An array of shape (n_queries, dim) containing the query vectors. \n",
    "        dim: The dimensionality of the vectors.\n",
    "    Returns:\n",
    "        An array of shape (n_queries, k) containing the indices of the k nearest neighbors for each query vector.\n",
    "    \"\"\"\n",
    "    ann_lists = []\n",
    "    for query_vec in query_vectors:\n",
    "        distances = np.linalg.norm(index_vectors - query_vec, axis=1)\n",
    "        ann_lists.append(list(np.argsort(distances)[:k]))\n",
    "    return np.array(ann_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4834b46dfb3911e5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def build_faiss_flatl2_index(\n",
    "        index_vectors: np.ndarray,\n",
    "        dim: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a Faiss flat L2 index.\n",
    "    Args:\n",
    "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
    "        dim: The dimensionality of the vectors. \n",
    "    Returns:\n",
    "        A Faiss flat L2 index.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(index_vectors)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ed56974e626fb2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def faiss_search(\n",
    "        query_vectors: np.ndarray,\n",
    "        index: faiss.Index,\n",
    "        k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function uses a Faiss index to search for the k-nearest neighbors of query_vectors.\n",
    "    Args:\n",
    "        query_vectors: An array of shape (n_queries, dim) containing the query vectors. \n",
    "        index: A Faiss index.\n",
    "        k: The number of nearest neighbors to retrieve.\n",
    "    Returns:\n",
    "        An array of shape (, ) containing the indices of the k-nearest neighbors for each query vector.\n",
    "    \"\"\"\n",
    "    distances, indices = index.search(query_vectors, k)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df4063f13bb9194",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def build_faiss_lsh_index(\n",
    "        index_vectors: np.ndarray,\n",
    "        dim: int,\n",
    "        nbits: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a Faiss LSH index.\n",
    "    Args:\n",
    "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
    "        dim: The dimensionality of the vectors. \n",
    "        nbits: The number of bits to use in the hash.\n",
    "    Returns:\n",
    "        A Faiss LSH index.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexLSH(dim, nbits)\n",
    "    index.add(index_vectors)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dc15876228fda5c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_recall_at_k(\n",
    "        nn_gt: np.ndarray,\n",
    "        ann: np.ndarray,\n",
    "        k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function computes the recall@k.\n",
    "    Args:\n",
    "        nn_gt: The ground truth nearest neighbors.\n",
    "        ann: The approximate nearest neighbors.\n",
    "        k: The number of nearest neighbors to consider.\n",
    "    Returns:\n",
    "        The recall@k.\n",
    "    \"\"\"\n",
    "    return round(sum([len(set(ann[i]) & set(nn_gt[i])) / k for i in range(len(ann))])/len(ann), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d4be2e90ed842",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2.1 -- LSH vs Naive Exhaustive Search (Regular Index Vectors)\n",
    "### You just have to run the following cells:\n",
    "* running time of the ground truth computation with semi_optimized_exhaustive_search (wall time)\n",
    "* running time of creating faiss_lsh_index (wall time)\n",
    "* running time of faiss_search over query_vectors with faiss_lsh_index (wall time)\n",
    "* recall@10 for faiss_lsh_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd9e16e708c498f8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "query_vectors = np.load('data/query_vectors.npy')\n",
    "index_vectors = np.load('data/index_vectors.npy')\n",
    "k = 10\n",
    "dim = index_vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "918d6bb76f333a49",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.86 s, sys: 42 ms, total: 8.91 s\n",
      "Wall time: 8.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gt_nn = semi_optimized_exhaustive_search(index_vectors, query_vectors, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfc9e299f7850ec8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 s, sys: 156 ms, total: 1.48 s\n",
      "Wall time: 824 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "faiss_lsh_index = build_faiss_lsh_index(index_vectors, dim, nbits=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53d4a791ae1682b0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.68 s, sys: 0 ns, total: 4.68 s\n",
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "faiss_lsh_ann = faiss_search(query_vectors, faiss_lsh_index, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0a48ef3-167e-403a-9ba7-72b4e3270ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(faiss_lsh_ann.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d99927576ef024",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@10 for faiss_lsh_index: 0.11\n"
     ]
    }
   ],
   "source": [
    "print(f\"recall@10 for faiss_lsh_index: {compute_recall_at_k(gt_nn, faiss_lsh_ann, k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ca983b3a893e5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2.2 -- Custom Indexing Algorithm\n",
    "Build an indexing algorithm that satisfies the following requirements:\n",
    "* The indexing algorithm should be able to handle vectors of different dimensions\n",
    "* The running time of the indexing should be less than half of the running time of semi_optimized_exhaustive_search), reported in Section 2.1.\n",
    "* The running time of searching over the index should be less than a third (1/3) of the time of the semi_optimized_exhaustive_search function, reported in Section 2.1.\n",
    "* The performance (in terms of recall@10) of the indexing algorithm should be at least 0.25.\n",
    "\n",
    "You are allowed to add as many helper functions as you need, as long as all of them appear in the next cell (the one containing custom_indexing_algorithm and custom_index_search). You cannot use faiss or scipy libraries for this task. Numpy is allowed. \n",
    "\n",
    "You can also test your algorithm with the additional two query-index sets by replacing the calls made few cells ago to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10d5d2d03736f99",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Write your code for 2.2.2 here\n",
    "# You are allowed to add more arguments to the functions and create more functions if needed.\n",
    "from collections import defaultdict\n",
    "\n",
    "def initialize_random_centroids(K, X):\n",
    "    \"\"\"Initializes and returns k random centroids\"\"\"\n",
    "    m, n = np.shape(X)\n",
    "    centroids = np.empty((K, n))\n",
    "    for i in range(K):\n",
    "        # pick a random data point from X as the centroid\n",
    "        centroids[i] =  X[np.random.choice(range(m))] \n",
    "    return centroids\n",
    "\n",
    "def initialize_plus_centroids(K, X):\n",
    "    \"\"\"k-means++ initialisation – spreads centroids, boosts recall.\"\"\"\n",
    "    m, n = X.shape\n",
    "    centroids = np.empty((K, n))\n",
    "    centroids[0] = X[np.random.choice(m)]          # 1st centre – random\n",
    "    dist2 = np.full(m, np.inf)                     # squared dists to nearest chosen\n",
    "    for i in range(1, K):\n",
    "        dist2 = np.minimum(dist2, np.sum((X - centroids[i-1])**2, axis=1))\n",
    "        probs = dist2 / dist2.sum()                # D² sampling\n",
    "        centroids[i] = X[np.random.choice(m, p=probs)]\n",
    "    return centroids\n",
    "\n",
    "def closest_centroid(x, centroids, K):\n",
    "    \"\"\"Finds and returns the index of the closest centroid for a given vector x\"\"\"\n",
    "    distances = np.empty(K)\n",
    "    for i in range(K):\n",
    "        # compute distance from each centroid to a data point\n",
    "        distances[i] = np.linalg.norm(centroids[i] - x)\n",
    "    return np.argmin(distances) # return the index of the closest centroid\n",
    "\n",
    "def assign_clusters(centroids, X):\n",
    "    \"\"\"Returns cluster indices for every sample.\"\"\"\n",
    "    dists = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n",
    "    return dists.argmin(axis=1)\n",
    "\n",
    "def compute_means(cluster_idx, K, X):\n",
    "    \"\"\"Computes and returns the new centroids of the clusters\"\"\"\n",
    "    _, n = np.shape(X)\n",
    "    centroids = np.empty((K, n))\n",
    "    for i in range(K):\n",
    "        points = X[cluster_idx == i]\n",
    "        if len(points) == 0:\n",
    "            centroids[i] = X[np.random.choice(len(X))]  # reinit\n",
    "        else:\n",
    "            centroids[i] = np.mean(points, axis=0)\n",
    "    return centroids\n",
    "    \n",
    "\n",
    "def run_k_means(K, X, max_iterations=100):\n",
    "    \"\"\"Runs the k-means algorithm and computes the final clusters\"\"\"\n",
    "    centroids = initialize_plus_centroids(K, X)  # initialize random centroids\n",
    "    for i in range(max_iterations):\n",
    "        clusters = assign_clusters(centroids, X)  # assign data points to centroids\n",
    "        previous_centroids = centroids.copy()                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "        centroids = compute_means(clusters, K, X)\n",
    "        diff = previous_centroids - centroids\n",
    "        if np.abs(diff).max() < 0.01:  # if there was no change - stop\n",
    "            break\n",
    "    return centroids, clusters\n",
    "\n",
    "\n",
    "def custom_indexing_algorithm(index_vectors, dim, n_clusters):\n",
    "    centroids, clusters = run_k_means(n_clusters, index_vectors)\n",
    "    \n",
    "    # build mapping from cluster_id to vectors\n",
    "    inverted_lists = defaultdict(list)\n",
    "    for i, cluster_id in enumerate(clusters.astype(int)):\n",
    "        inverted_lists[cluster_id].append((index_vectors[i], i))  # store tuple: (vector, original index)\n",
    "    \n",
    "    for cluster_id in inverted_lists:\n",
    "        vectors, indices = zip(*inverted_lists[cluster_id])\n",
    "        inverted_lists[cluster_id] = (np.vstack(vectors), np.array(indices))\n",
    "        \n",
    "    return centroids, inverted_lists\n",
    "\n",
    "def _gather_candidates(q, C, inv, L):\n",
    "    d   = np.linalg.norm(C - q, axis=1)          # dist to each centroid\n",
    "    cand = np.argsort(d)[:L]                # only top L centroids\n",
    "    Vlst, Ilst = [], []\n",
    "    for cid in cand:                             # build candidate pool\n",
    "        if cid in inv:\n",
    "            v, idx = inv[cid]\n",
    "            Vlst.append(v)\n",
    "            Ilst.append(idx)\n",
    "    if not Vlst:                                 # empty handling\n",
    "        return None, None\n",
    "    return np.vstack(Vlst), np.concatenate(Ilst)\n",
    "\n",
    "\n",
    "def custom_index_search(query_vectors, index, k, L=2):\n",
    "    C, inv = index                       \n",
    "    out = []\n",
    "    for q in query_vectors:              # process each query \n",
    "        V, I = _gather_candidates(q, C, inv, L)  # pool of candidate vectors,ids\n",
    "        if V is None:                    # no vectors found (empty clusters)\n",
    "            out.append([])\n",
    "            continue\n",
    "        # rank all candidates by Euclidean distance to the query\n",
    "        best = I[np.argsort(np.linalg.norm(V - q, axis=1))[:k]]\n",
    "        out.append(list(best))           # store the top-k original indices\n",
    "    return np.array(out)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43b8fb6b0ca10d37",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add hyperparameters here (if needed)\n",
    "N_CLUSTERS = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef371ecd242846db",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 320 ms, sys: 948 ms, total: 1.27 s\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "custom_index = custom_indexing_algorithm(index_vectors, dim, N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c40c61275a3d001",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.39 s, sys: 8.03 ms, total: 2.39 s\n",
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "custom_index_ann = custom_index_search(query_vectors, custom_index, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92f130cfac782571",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@10 for custom_index_search: 0.909\n"
     ]
    }
   ],
   "source": [
    "print(f\"recall@10 for custom_index_search: {compute_recall_at_k(gt_nn, custom_index_ann, k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3efb8-da47-4088-ba1c-c66cedfac704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
